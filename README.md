## Empirical Analysis of Performance Evaluation for Imbalanced Classification

This repository contains all the code used in the empirical tests conducted in the paper *"Empirical Analysis of Performance Evaluation for Imbalanced Classification"* [1].

This repository is organized as follows:

* **PerformanceEvaluationImbalancedClassification.ipynb** file - contains all the code for reproducing the experiments presented in the paper;
* **Data** folder - contains the data sets used in the experiments carried out, taken from the KEEL repository [2];

### Requirements
The experiments were implemented in Python within the Jupyter Notebook environment. The code and results can be seen by opening the PerformanceEvaluationImbalancedClassification.ipynb notebook.

If you want to replicate the experiments by yourself, a working installation of Python (https://www.python.org/) and Jupyter Notebook (https://jupyter.org/) are required.

Furthermore, the following Python packages are required:

- Pandas (https://pypi.org/project/pandas/)
- Numpy (https://pypi.org/project/numpy/)
- Matplotlib (https://pypi.org/project/matplotlib/)
- Krippendorff (https://pypi.org/project/krippendorff/)
- Scikit-Learn (https://pypi.org/project/scikit-learn/)
- Pyprg (https://pypi.org/project/pyprg/)
- HMeasure (https://pypi.org/project/hmeasure/)
- SciPy (https://pypi.org/project/scipy/)

### References
[1] Gaudreault, J. G., & Branco, P. (2024). Empirical analysis of performance assessment for imbalanced classification. Machine Learning, 1-43.

[2] Alcalá-Fdez, J., Fernandez, A., Luengo, J., Derrac J., García S., Sánchez L., Herrera, F. (2011) KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework. Journal of Multiple-Valued Logic and Soft Computing.
